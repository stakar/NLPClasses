{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory of automatic natural language processing \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is notebook presenting results from NLP based study. \n",
    "\n",
    "THe main aim was creating texts classifier. Firstly, dataset for training was acquired\n",
    "using webscrapper, then data was processed in order to reject dirty instances. After that,\n",
    "the features of text was extracted. \n",
    "\n",
    "Each feature was word present in dataset, which was counted for each instance and then\n",
    "the number of occurances of it. It was returned as vector of words.\n",
    "\n",
    "After that, the classifier using Naive Bayes algorithm was trained and tested on\n",
    "achieved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapper object created!\n",
      "Processing to:  movie\n",
      "Processing to:  Star-Trek\n",
      "0.005135999999993146\n",
      "Filtering and preparing links:  DONE\n",
      "Scrapper will now download the fanfiction into files. \n",
      "\n",
      "=======================================================\n",
      "Status:\n",
      "Links on website: 25\n",
      "Links on HDD: 0 \n",
      "\n",
      "You have 25 links left to scrap.\n",
      "=======================================================\n",
      "Do you want to continue? [Y/N] \n",
      "y\n",
      "How many links(stories) you want to download? \n",
      " \n",
      "all or int \n",
      "all\n",
      "Operation completed!\n"
     ]
    }
   ],
   "source": [
    "#Importing modules\n",
    "from scraperFanFic import *\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer,\n",
    "                                             CountVectorizer)\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "from nltk.text import Text\n",
    "from nltk.text import ContextIndex\n",
    "from nltk.text import ConcordanceIndex\n",
    "\n",
    "SITE = \"http://fanfiction.net\"  # Default site to work on, tested.\n",
    "from scraperFanFic import *\n",
    "CATEGORY = 'movie'\n",
    "UNIVERSE = 'Star-Trek'\n",
    "scrap = Scrapper(SITE)\n",
    "print('Scrapper object created!')\n",
    "scrap.follow_link(CATEGORY)\n",
    "print('Processing to: ', CATEGORY)\n",
    "scrap.follow_link(UNIVERSE)\n",
    "print('Processing to: ', UNIVERSE)\n",
    "scrap.open(scrap.filtered_fan_fiction())\n",
    "\n",
    "start = timer()\n",
    "scrap.preparing_links()\n",
    "end = timer()\n",
    "print(end - start)\n",
    "\n",
    "print(\"Filtering and preparing links: \", 'DONE')\n",
    "print(\"Scrapper will now download the fanfiction into files. \\n\")\n",
    "print('=======================================================')\n",
    "print(\"Status:\")\n",
    "print(\"Links on website:\", len(scrap.linksOnPage))\n",
    "print(\"Links on HDD:\", len(scrap.linksOnPage) - len(scrap.checkfiles()), \"\\n\")\n",
    "print(\"You have {} links left to scrap.\".format(len(scrap.linksToDownload)))\n",
    "print('=======================================================')\n",
    "\n",
    "answer = input(\"Do you want to continue? [Y/N] \\n\")\n",
    "if answer.upper() == \"Y\":\n",
    "\n",
    "    print('How many links(stories) you want to download? \\n ')\n",
    "    x = input(\"all or int \\n\")\n",
    "    if x.upper() == \"ALL\":\n",
    "        story = scrap.download_list(ran_giv=len(scrap.linksToDownload))\n",
    "    else:\n",
    "        story = scrap.download_list(ran_giv=int(x))\n",
    "if answer.upper() == 'N':\n",
    "    print('Do you want to create a dataset?')\n",
    "    n = input('[Y/N] \\n')\n",
    "    if n.upper() == \"Y\":\n",
    "        story = scrap.read_stories_HDD()\n",
    "    elif n.upper() == 'N':\n",
    "        print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 6919521\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "with open('text.txt','r') as txt:\n",
    "    text = txt.read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretest\n",
    "pretest = text[:1000000]\n",
    "text = pretest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 333320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\stakar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\stakar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "333320/333320 [==============================] - 249s 747us/step - loss: 1.8512\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"her was everything the evil man had ever\"\n",
      "her was everything the evil man had everyone was to the she would real wont to and they wont to the she would not their command their contrown that he was to their stand with the some on the she would really and they would really work and they wont to their contring to have and they would have to their stand to their faces with the she was to their contrown to their face with the she was to the ship on the ship on the she would have to \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"her was everything the evil man had ever\"\n",
      "her was everything the evil man had every can jought want to other to their was nonning a right eaba, and their tonge and from their face contrister of their full want and stand them with so make a promis to won the starder when he said would real would not them to the she was head how spock wont to any the standing proin on thing and condering to the she was to to see of their surrys to the she and she was dood her hard how remory and \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"her was everything the evil man had ever\"\n",
      "her was everything the evil man had every helped.s was shibluse, Margareed, his gagged drow, work. He get really lecond contic reor I wont aunty day.-You nond that we less form distoun orrit rein Sam khos back a fact out with yo and Stargate I , have in lycent as never calse. Nook dances wouldreent mome like creets, Mama canding I sure hin of she a was toamn bany sole. Grayson studeed are expect keepled of dinger and smile of wasy when \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"her was everything the evil man had ever\"\n",
      "her was everything the evil man had every conttionaYes I cloied, and then 8m crin herilical skix to usvillf. Atomensiy chirmed, hed towny quirter than In nell speck.Threough, shrated to t ok, only head someten arroSpocisy, and is ryodd of wer.Is proitals winle to. He was tryeKlipt mat, do, fallowevered brobsepor toreeted. Kokes. in asken.,  eun waenty., Jimmandling to levidI anmoking for for made arojulntous vin wall. Telion was Scomm -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22cc2929a58>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "# filename = 'finalized_model2.sav'\n",
    "# pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               107008    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                10320     \n",
      "=================================================================\n",
      "Total params: 117,328\n",
      "Trainable params: 117,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.7]:\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "#     print('----- Generating with seed: \"' + sentence + '\"')\n",
    "#     sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(1000):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "#         sys.stdout.write(next_char)\n",
    "#         sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pre = pickle.load(open('finalized_model.sav', 'rb'))\n",
    "\n",
    "neural_texts = list()\n",
    "fan_fic_len = 400\n",
    "\n",
    "for n in range(10):\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.7]:\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "\n",
    "        for i in range(fan_fic_len):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    neural_texts.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_texts = list()\n",
    "\n",
    "for n in range(10):\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    chosen = pretest[start_index:start_index+maxlen+fan_fic_len]\n",
    "    random_texts.append(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('neuralText.txt','w') as nt:\n",
    "    for n in range(10):\n",
    "        nt.write(neural_texts[n] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('randomText.txt','w') as rt:\n",
    "    for n in range(10):\n",
    "        rt.write(random_texts[n] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
