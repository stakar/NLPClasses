{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraperFanFic import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "def array_words(stories_list):\n",
    "    count = CountVectorizer(analyzer='word',tokenizer=None,\n",
    "                      preprocessor = None,max_features = 5000)\n",
    "    stories = preparing_stories(stories_list)\n",
    "    transformed = count.fit_transform(stories)\n",
    "    feature_array = transformed.toarray()\n",
    "    return feature_array\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preparing_stories(stories_list):\n",
    "    result_list = list()\n",
    "    for n in stories_list:\n",
    "        result = re.sub(\"[^a-zA-Z]\",\" \",n)\n",
    "        result = result.lower()\n",
    "        result = result.split(' ')\n",
    "        result = [w for w in result if w not in stopwords.words('english')]\n",
    "        result = ' '.join(result)\n",
    "        result_list.append(result)\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapper object created!\n",
      "Processing to:  movie\n",
      "Processing to:  Star-Wars\n",
      "0.009807200000068406\n",
      "Filtering and preparing links:  DONE\n",
      "Scrapper will now download the fanfiction into files. \n",
      "\n",
      "=======================================================\n",
      "Status:\n",
      "Links on website: 25\n",
      "Links on HDD: 6 \n",
      "\n",
      "You have 19 links left to scrap.\n",
      "=======================================================\n",
      "Do you want to continue? [Y/N] \n",
      "y\n",
      "How many links(stories) you want to download? \n",
      " \n",
      "all or int \n",
      "2\n",
      "Operation completed!\n",
      "Scrapper object created!\n",
      "Processing to:  movie\n",
      "Processing to:  Star-Trek\n",
      "0.006131399999958376\n",
      "Filtering and preparing links:  DONE\n",
      "Scrapper will now download the fanfiction into files. \n",
      "\n",
      "=======================================================\n",
      "Status:\n",
      "Links on website: 25\n",
      "Links on HDD: 3 \n",
      "\n",
      "You have 22 links left to scrap.\n",
      "=======================================================\n",
      "Do you want to continue? [Y/N] \n",
      "y\n",
      "How many links(stories) you want to download? \n",
      " \n",
      "all or int \n",
      "2\n",
      "Operation completed!\n"
     ]
    }
   ],
   "source": [
    "dataset0,dataset1 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset0 = np.array([dataset0]).squeeze()\n",
    "dataset1 = np.array([dataset1]).squeeze()\n",
    "target0 = np.array([0 for n in range(len(dataset0))])\n",
    "target1 = np.array([1 for n in range(len(dataset1))])\n",
    "data = np.concatenate([dataset0,dataset1])\n",
    "target = np.concatenate([target0,target1])\n",
    "\n",
    "processed0 = array_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(processed0,target)\n",
    "gb = GaussianNB()\n",
    "gb.fit(X_train,y_train)\n",
    "gb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_oneway\n",
    "\n",
    "anova = f_oneway(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    SITE = \"http://fanfiction.net\"  # Default site to work on, tested.\n",
    "\n",
    "    CATEGORY = 'movie'\n",
    "    UNIVERSE = 'Star-Wars'\n",
    "    scrap = Scrapper(SITE)\n",
    "    print('Scrapper object created!')\n",
    "    scrap.follow_link(CATEGORY)\n",
    "    print('Processing to: ', CATEGORY)\n",
    "    scrap.follow_link(UNIVERSE)\n",
    "    print('Processing to: ', UNIVERSE)\n",
    "    scrap.open(scrap.filtered_fan_fiction())\n",
    "\n",
    "    start = timer()\n",
    "    scrap.preparing_links()\n",
    "    end = timer()\n",
    "    print(end - start)\n",
    "\n",
    "    print(\"Filtering and preparing links: \", 'DONE')\n",
    "    print(\"Scrapper will now download the fanfiction into files. \\n\")\n",
    "    print('=======================================================')\n",
    "    print(\"Status:\")\n",
    "    print(\"Links on website:\", len(scrap.linksOnPage))\n",
    "    print(\"Links on HDD:\", len(scrap.linksOnPage) - len(scrap.checkfiles()), \"\\n\")\n",
    "    print(\"You have {} links left to scrap.\".format(len(scrap.linksToDownload)))\n",
    "    print('=======================================================')\n",
    "\n",
    "    answer = input(\"Do you want to continue? [Y/N] \\n\")\n",
    "    if answer.upper() == \"Y\":\n",
    "\n",
    "        print('How many links(stories) you want to download? \\n ')\n",
    "        x = input(\"all or int \\n\")\n",
    "        if x.upper() == \"ALL\":\n",
    "            story = scrap.download_list(ran_giv=len(scrap.linksToDownload))\n",
    "        else:\n",
    "            story = scrap.download_list(ran_giv=int(x))\n",
    "    if answer.upper() == 'N':\n",
    "        print('Do you want to create a dataset?')\n",
    "        n = input('[Y/N] \\n')\n",
    "        if n.upper() == \"Y\":\n",
    "            story = scrap.read_stories_HDD()\n",
    "        elif n.upper() == 'N':\n",
    "            print('Done!')\n",
    "\n",
    "    predataset0 = [[p[n][0] for n in range(len(p)) if len(p[n]) == 2] for p in story]\n",
    "    dataset0 = [r for t in predataset0 for r in t]\n",
    "\n",
    "\n",
    "    SITE = \"http://fanfiction.net\"  # Default site to work on, tested.\n",
    "#     from scraperFanFic import *\n",
    "    CATEGORY = 'movie'\n",
    "    UNIVERSE = 'Star-Trek'\n",
    "    scrap = Scrapper(SITE)\n",
    "    print('Scrapper object created!')\n",
    "    scrap.follow_link(CATEGORY)\n",
    "    print('Processing to: ', CATEGORY)\n",
    "    scrap.follow_link(UNIVERSE)\n",
    "    print('Processing to: ', UNIVERSE)\n",
    "    scrap.open(scrap.filtered_fan_fiction())\n",
    "\n",
    "    start = timer()\n",
    "    scrap.preparing_links()\n",
    "    end = timer()\n",
    "    print(end - start)\n",
    "\n",
    "    print(\"Filtering and preparing links: \", 'DONE')\n",
    "    print(\"Scrapper will now download the fanfiction into files. \\n\")\n",
    "    print('=======================================================')\n",
    "    print(\"Status:\")\n",
    "    print(\"Links on website:\", len(scrap.linksOnPage))\n",
    "    print(\"Links on HDD:\", len(scrap.linksOnPage) - len(scrap.checkfiles()), \"\\n\")\n",
    "    print(\"You have {} links left to scrap.\".format(len(scrap.linksToDownload)))\n",
    "    print('=======================================================')\n",
    "\n",
    "    answer = input(\"Do you want to continue? [Y/N] \\n\")\n",
    "    if answer.upper() == \"Y\":\n",
    "\n",
    "        print('How many links(stories) you want to download? \\n ')\n",
    "        x = input(\"all or int \\n\")\n",
    "        if x.upper() == \"ALL\":\n",
    "            story = scrap.download_list(ran_giv=len(scrap.linksToDownload))\n",
    "        else:\n",
    "            story = scrap.download_list(ran_giv=int(x))\n",
    "    if answer.upper() == 'N':\n",
    "        print('Do you want to create a dataset?')\n",
    "        n = input('[Y/N] \\n')\n",
    "        if n.upper() == \"Y\":\n",
    "            story = scrap.read_stories_HDD()\n",
    "        elif n.upper() == 'N':\n",
    "            print('Done!')\n",
    "\n",
    "    predataset1 = [[p[n][0] for n in range(len(p)) if len(p[n]) == 2] for p in story]\n",
    "    dataset1 = [r for t in predataset1 for r in t]\n",
    "    return dataset0,dataset1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
