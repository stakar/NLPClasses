{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory of automatic natural language processing \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is notebook presenting results from NLP based study. \n",
    "\n",
    "THe main aim was creating texts classifier. Firstly, dataset for training was acquired\n",
    "using webscrapper, then data was processed in order to reject dirty instances. After that,\n",
    "the features of text was extracted. \n",
    "\n",
    "Each feature was word present in dataset, which was counted for each instance and then\n",
    "the number of occurances of it. It was returned as vector of words.\n",
    "\n",
    "After that, the classifier using Naive Bayes algorithm was trained and tested on\n",
    "achieved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing modules\n",
    "from scraperFanFic import *\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer,\n",
    "                                             CountVectorizer)\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "from nltk.text import Text\n",
    "from nltk.text import ContextIndex\n",
    "from nltk.text import ConcordanceIndex\n",
    "\n",
    "#Defining function for cleaning data\n",
    "def preparing_stories(stories_list):\n",
    "    result_list = list()\n",
    "    for n in stories_list:\n",
    "        #Getting rid of interpunction\n",
    "        result = re.sub(\"[^a-zA-Z]\",\" \",n)\n",
    "        #Lowering data to avoid problems wit case sensitivity\n",
    "        result = result.lower()\n",
    "        #Creating list of all words\n",
    "        result = result.split(' ')\n",
    "        #Rejecting words that occurs too often\n",
    "        result = [w for w in result if w not in stopwords.words('english')]\n",
    "        #Joining words into one string\n",
    "        result = ' '.join(result)\n",
    "        #Appending processed data to results\n",
    "        result_list.append(result)\n",
    "    return result_list\n",
    "\n",
    "def aquire_freq_dist(stories_list):\n",
    "    result_list = list()\n",
    "    for n in stories_list:\n",
    "        #Getting rid of interpunction\n",
    "        result = re.sub(\"[^a-zA-Z]\",\" \",n)\n",
    "        #Lowering data to avoid problems wit case sensitivity\n",
    "        result = result.lower()\n",
    "        #Creating list of all words\n",
    "        result = result.split(' ')\n",
    "        #Rejecting words that occurs too often\n",
    "        result = [w for w in result if w not in stopwords.words('english')]\n",
    "        #Joining words into one string\n",
    "        for w in result:\n",
    "            result_list.append(w)\n",
    "    return FreqDist(result_list)\n",
    "\n",
    "\n",
    "def aquire_list(stories_list):\n",
    "    result_list = list()\n",
    "    for n in stories_list:\n",
    "        #Getting rid of interpunction\n",
    "#         result = re.sub(\"[^a-zA-Z]\",\" \",n)\n",
    "        #Lowering data to avoid problems wit case sensitivity\n",
    "#         result = n.lower()\n",
    "        #Creating list of all words\n",
    "        result = n.split(' ')\n",
    "        #Rejecting words that occurs too often\n",
    "#         result = [w for w in result if w not in stopwords.words('english')]\n",
    "        #Joining words into one string\n",
    "        for w in result:\n",
    "            result_list.append(w)\n",
    "    return result_list\n",
    "\n",
    "def array_words(stories_list):\n",
    "    #Calling vectorizer\n",
    "    count = CountVectorizer(analyzer='word',tokenizer=None,\n",
    "                      preprocessor = None,max_features = 5000)\n",
    "    stories = preparing_stories(stories_list)\n",
    "    transformed = count.fit_transform(stories)\n",
    "    feature_array = transformed.toarray()\n",
    "    feature_name = count.get_feature_names()\n",
    "    return feature_name,feature_array\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting datasets, 0 for Star Wars and 1 for Star Trek\n",
    "#Code for main function is defined at the bottom of notebook,\n",
    "#for aesthetic purposes.\n",
    "# # dataset0,dataset1 = main()\n",
    "X_train,X_test,y_test,y_train = main()\n",
    "\n",
    "unprocessed = np.concatenate([X_train,X_test])\n",
    "y = np.concatenate([y_train,y_test])\n",
    "names,processed0 = array_words(unprocessed) \n",
    "\n",
    "# swdata = [X_train[n] for n in range(len(X_train)) if y_train[n] != 1]\n",
    "# stdata = [X_train[n] for n in range(len(X_train)) if y_train[n] != 0]\n",
    "\n",
    "# evenlist = np.concatenate([swdata[:200],stdata[:200]])\n",
    "\n",
    "# sw_words = aquire_list(X_train)\n",
    "\n",
    "# textsw = Text( sw_words)\n",
    "# textst = Text(' '.join(stdata))\n",
    "\n",
    "# even_words = aquire_list(evenlist)\n",
    "# even_dist = aquire_freq_dist(evenlist)\n",
    "# even_text = Text(even_words)\n",
    "\n",
    "# first_words = np.array(even_dist.most_common()[1:][:10])[:,0]\n",
    "# for n in first_words:\n",
    "#     print(even_text.concordance(n,lines = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifier\n",
    "\n",
    "Now we are learning classifier and getting the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958847736625515"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(processed0,y)\n",
    "gb = GaussianNB()\n",
    "gb.fit(X_train,y_train)\n",
    "gb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Star Wars</th>\n",
       "      <th>Star Trek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision for class</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall for class</th>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F score</th>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of instances</th>\n",
       "      <td>95.00</td>\n",
       "      <td>148.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Star Wars  Star Trek\n",
       "Precision for class       1.00       0.99\n",
       "Recall for class          0.99       1.00\n",
       "F score                   0.99       1.00\n",
       "Number of instances      95.00     148.00"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "prec = precision_recall_fscore_support(y_test,gb.predict(X_test))\n",
    "prec[0]\n",
    "dict_res = {'Precision for class':np.round(prec[0],2),\n",
    "            'Recall for class':np.round(prec[1],2),\n",
    "            'F score':np.round(prec[2],2),\n",
    "            'Number of instances':np.round(prec[3])}\n",
    "results = pd.DataFrame(data=dict_res).T\n",
    "results.columns = ['Star Wars','Star Trek']\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open('testdata.txt','r') as test:\n",
    "        X_test = test.read()\n",
    "    \n",
    "    with open('traindata.txt','r') as train:\n",
    "        X_train = train.read()\n",
    "    datatest = [n[9:] for n in X_train.split('\\n')][:-1]\n",
    "    datatrain = [n[9:] for n in X_test.split('\\n')][:-1]\n",
    "        \n",
    "    dict_mapped = {'StarWars':0,'StarTrek':1}\n",
    "    tartrain = [dict_mapped[X_train.split('\\n')[n][:8]] for n in range(len(X_train.split('\\n'))-1)]\n",
    "    tartest = [dict_mapped[X_test.split('\\n')[n][:8]] for n in range(len(X_test.split('\\n'))-1)]\n",
    "    return datatrain,datatest,tartrain,tartest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
