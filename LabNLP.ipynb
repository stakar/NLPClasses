{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory of automatic natural language processing \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is notebook presenting results from NLP based study. \n",
    "\n",
    "THe main aim was creating texts classifier. Firstly, dataset for training was acquired\n",
    "using webscrapper, then data was processed in order to reject dirty instances. After that,\n",
    "the features of text was extracted. \n",
    "\n",
    "Each feature was word present in dataset, which was counted for each instance and then\n",
    "the number of occurances of it. It was returned as vector of words.\n",
    "\n",
    "After that, the classifier using Naive Bayes algorithm was trained and tested on\n",
    "achieved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing modules\n",
    "from scraperFanFic import *\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer,\n",
    "                                             CountVectorizer)\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "#Defining function for cleaning data\n",
    "def preparing_stories(stories_list):\n",
    "    result_list = list()\n",
    "    for n in stories_list:\n",
    "        #Getting rid of interpunction\n",
    "        result = re.sub(\"[^a-zA-Z]\",\" \",n)\n",
    "        #Lowering data to avoid problems wit case sensitivity\n",
    "        result = result.lower()\n",
    "        #Creating list of all words\n",
    "        result = result.split(' ')\n",
    "        #Rejecting words that occurs too often\n",
    "        result = [w for w in result if w not in stopwords.words('english')]\n",
    "        #Joining words into one string\n",
    "        result = ' '.join(result)\n",
    "        #Appending processed data to results\n",
    "        result_list.append(result)\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def array_words(stories_list):\n",
    "    #Calling vectorizer\n",
    "    count = CountVectorizer(analyzer='word',tokenizer=None,\n",
    "                      preprocessor = None,max_features = 5000)\n",
    "    stories = preparing_stories(stories_list)\n",
    "    transformed = count.fit_transform(stories)\n",
    "    feature_array = transformed.toarray()\n",
    "    return feature_array\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapper object created!\n",
      "Processing to:  movie\n",
      "Processing to:  Star-Wars\n",
      "0.00932039999997869\n",
      "Filtering and preparing links:  DONE\n",
      "Scrapper will now download the fanfiction into files. \n",
      "\n",
      "=======================================================\n",
      "Status:\n",
      "Links on website: 25\n",
      "Links on HDD: 0 \n",
      "\n",
      "You have 25 links left to scrap.\n",
      "=======================================================\n",
      "Do you want to continue? [Y/N] \n",
      "y\n",
      "How many links(stories) you want to download? \n",
      " \n",
      "all or int \n",
      "6\n",
      "Operation completed!\n",
      "Scrapper object created!\n",
      "Processing to:  movie\n",
      "Processing to:  Star-Trek\n",
      "0.0058641000000534405\n",
      "Filtering and preparing links:  DONE\n",
      "Scrapper will now download the fanfiction into files. \n",
      "\n",
      "=======================================================\n",
      "Status:\n",
      "Links on website: 25\n",
      "Links on HDD: 0 \n",
      "\n",
      "You have 25 links left to scrap.\n",
      "=======================================================\n",
      "Do you want to continue? [Y/N] \n",
      "y\n",
      "How many links(stories) you want to download? \n",
      " \n",
      "all or int \n",
      "6\n",
      "Operation completed!\n"
     ]
    }
   ],
   "source": [
    "#Getting datasets, 0 for Star Wars and 1 for Star Trek\n",
    "#Code for main function is defined at the bottom of notebook,\n",
    "#for aesthetic purposes.\n",
    "dataset0,dataset1 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Star Wars instances: 118\n",
      "Number of Star Trek instances: 521\n",
      "Shape of dataset: (639,)\n",
      "Shape of target vector: (639,)\n",
      "Shape of feature dataset(639, 5000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Turn list into element-wise array\n",
    "dataset0 = np.array([dataset0]).squeeze()\n",
    "dataset1 = np.array([dataset1]).squeeze()\n",
    "#Star Wars is mapped as 0\n",
    "target0 = np.array([0 for n in range(len(dataset0))])\n",
    "#Star Trek is mapped as 1\n",
    "target1 = np.array([1 for n in range(len(dataset1))])\n",
    "print('Number of Star Wars instances: {}'.format(target0.shape[0]))\n",
    "print('Number of Star Trek instances: {}'.format(target1.shape[0]))\n",
    "#Creating dataset and target vector\n",
    "data = np.concatenate([dataset0,dataset1])\n",
    "target = np.concatenate([target0,target1])\n",
    "\n",
    "print('Shape of dataset: {}'.format(data.shape))\n",
    "print('Shape of target vector: {}'.format(target.shape))\n",
    "\n",
    "#Turning them into features array\n",
    "processed0 = array_words(data)\n",
    "print('Shape of feature dataset{}'.format(processed0.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifier\n",
    "\n",
    "Now we are learning classifier and getting the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98125"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(processed0,target)\n",
    "gb = GaussianNB()\n",
    "gb.fit(X_train,y_train)\n",
    "gb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    SITE = \"http://fanfiction.net\"  # Default site to work on, tested.\n",
    "\n",
    "    CATEGORY = 'movie'\n",
    "    UNIVERSE = 'Star-Wars'\n",
    "    scrap = Scrapper(SITE)\n",
    "    print('Scrapper object created!')\n",
    "    scrap.follow_link(CATEGORY)\n",
    "    print('Processing to: ', CATEGORY)\n",
    "    scrap.follow_link(UNIVERSE)\n",
    "    print('Processing to: ', UNIVERSE)\n",
    "    scrap.open(scrap.filtered_fan_fiction())\n",
    "\n",
    "    start = timer()\n",
    "    scrap.preparing_links()\n",
    "    end = timer()\n",
    "    print(end - start)\n",
    "\n",
    "    print(\"Filtering and preparing links: \", 'DONE')\n",
    "    print(\"Scrapper will now download the fanfiction into files. \\n\")\n",
    "    print('=======================================================')\n",
    "    print(\"Status:\")\n",
    "    print(\"Links on website:\", len(scrap.linksOnPage))\n",
    "    print(\"Links on HDD:\", len(scrap.linksOnPage) - len(scrap.checkfiles()), \"\\n\")\n",
    "    print(\"You have {} links left to scrap.\".format(len(scrap.linksToDownload)))\n",
    "    print('=======================================================')\n",
    "\n",
    "    answer = input(\"Do you want to continue? [Y/N] \\n\")\n",
    "    if answer.upper() == \"Y\":\n",
    "\n",
    "        print('How many links(stories) you want to download? \\n ')\n",
    "        x = input(\"all or int \\n\")\n",
    "        if x.upper() == \"ALL\":\n",
    "            story = scrap.download_list(ran_giv=len(scrap.linksToDownload))\n",
    "        else:\n",
    "            story = scrap.download_list(ran_giv=int(x))\n",
    "    if answer.upper() == 'N':\n",
    "        print('Do you want to create a dataset?')\n",
    "        n = input('[Y/N] \\n')\n",
    "        if n.upper() == \"Y\":\n",
    "            story = scrap.read_stories_HDD()\n",
    "        elif n.upper() == 'N':\n",
    "            print('Done!')\n",
    "\n",
    "    predataset0 = [[p[n][0] for n in range(len(p)) if len(p[n]) == 2] for p in story]\n",
    "    dataset0 = [r for t in predataset0 for r in t]\n",
    "\n",
    "\n",
    "    SITE = \"http://fanfiction.net\"  # Default site to work on, tested.\n",
    "#     from scraperFanFic import *\n",
    "    CATEGORY = 'movie'\n",
    "    UNIVERSE = 'Star-Trek'\n",
    "    scrap = Scrapper(SITE)\n",
    "    print('Scrapper object created!')\n",
    "    scrap.follow_link(CATEGORY)\n",
    "    print('Processing to: ', CATEGORY)\n",
    "    scrap.follow_link(UNIVERSE)\n",
    "    print('Processing to: ', UNIVERSE)\n",
    "    scrap.open(scrap.filtered_fan_fiction())\n",
    "\n",
    "    start = timer()\n",
    "    scrap.preparing_links()\n",
    "    end = timer()\n",
    "    print(end - start)\n",
    "\n",
    "    print(\"Filtering and preparing links: \", 'DONE')\n",
    "    print(\"Scrapper will now download the fanfiction into files. \\n\")\n",
    "    print('=======================================================')\n",
    "    print(\"Status:\")\n",
    "    print(\"Links on website:\", len(scrap.linksOnPage))\n",
    "    print(\"Links on HDD:\", len(scrap.linksOnPage) - len(scrap.checkfiles()), \"\\n\")\n",
    "    print(\"You have {} links left to scrap.\".format(len(scrap.linksToDownload)))\n",
    "    print('=======================================================')\n",
    "\n",
    "    answer = input(\"Do you want to continue? [Y/N] \\n\")\n",
    "    if answer.upper() == \"Y\":\n",
    "\n",
    "        print('How many links(stories) you want to download? \\n ')\n",
    "        x = input(\"all or int \\n\")\n",
    "        if x.upper() == \"ALL\":\n",
    "            story = scrap.download_list(ran_giv=len(scrap.linksToDownload))\n",
    "        else:\n",
    "            story = scrap.download_list(ran_giv=int(x))\n",
    "    if answer.upper() == 'N':\n",
    "        print('Do you want to create a dataset?')\n",
    "        n = input('[Y/N] \\n')\n",
    "        if n.upper() == \"Y\":\n",
    "            story = scrap.read_stories_HDD()\n",
    "        elif n.upper() == 'N':\n",
    "            print('Done!')\n",
    "\n",
    "    predataset1 = [[p[n][0] for n in range(len(p)) if len(p[n]) == 2] for p in story]\n",
    "    dataset1 = [r for t in predataset1 for r in t]\n",
    "    return dataset0,dataset1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
